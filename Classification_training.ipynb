{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import datasets\n",
    "from sklearn.multiclass import OutputCodeClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import LeavePOut\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Training\n",
    "\n",
    "- Data format: The data format for x (feature matrix) is in the general form of (m*n) where m is the length of data, and n is the numder of features. Here the features are the mean values of each FSR and the x and y values of the accelerometer. Labels are shown by vector y1. Detail related to data can be found under Section 3.2. \n",
    "\n",
    "- Hyperparameter Tuning: details of all possible choices can be found in Section 4.1\n",
    "\n",
    "- Class \"Classifier_hypertuning\" can be used for hyperparameter tuning of the random forest and support vector machine algorithms. \n",
    "- Class \"Classifier_training\" can be used for training the algorithms. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter choice for the Classifier_hypertuning class\n",
    "\n",
    "- min_samples_split: type required is a python list of integers. \n",
    "- min_samples_leaf : type required is a python list of integers.\n",
    "- bootstrap: a bolean list: [True, False]\n",
    "- start-tree, stop_tree, num_tree, min_depth, max_depth, num_depth, n_iter, cv: integer\n",
    "- norm_condition: should be set to either False or True.\n",
    "- list_C: type required is a python list of integers.\n",
    "- list_gamma:type required is a python list of floats.  \n",
    "\n",
    "# Explanaition of the Functions in the class\n",
    "\n",
    "- RF_hyper_parameter_setup(self,x): used to set up all the possible hyperparameter values for the tuning\n",
    "- RF_hyper_parameter_tuning(self,x,y): used to find the optimzed hyperparameter values for RF.\n",
    "- SVM_hyperparameter_tuning(self,x,y): used to find the optimzed hyperparameter values for SVC. \n",
    "- LOOV(self,x,y): used to make training and test data sets based on leave one out validation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is *ONLY* as an exmaple. \n",
    "min_samples_split = [2, 5, 7, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "# Create the random grid\n",
    "start_tree = 5\n",
    "stop_tree = 100\n",
    "num_tree = 10\n",
    "min_depth = 2\n",
    "max_depth = 10\n",
    "num_depth = 2\n",
    "n_iter = 500\n",
    "cv = 5\n",
    "norm_condition = 'False'\n",
    "list_C = [0.1, 1, 10, 100, 1000]\n",
    "list_gamma = [1, 0.1, 0.01, 0.001, 0.0001]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier_hypertuning:\n",
    "    \n",
    "    def __init__(self,min_samples_split,min_samples_leaf,bootstrap,start_tree,stop_tree,num_tree,min_depth,max_depth,num_depth,n_iter,cv,norm_condition, list_C, list_gamma):\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.bootstrap = bootstrap\n",
    "        self.start_tree = start_tree\n",
    "        self.stop_tree = stop_tree\n",
    "        self.num_tree = num_tree\n",
    "        self.min_depth = min_depth\n",
    "        self.max_depth = max_depth\n",
    "        self.num_depth = num_depth\n",
    "        self.n_iter = n_iter\n",
    "        self.cv = cv\n",
    "        self.norm_condition = norm_condition # string of form 'False' or 'True'\n",
    "        self.list_C = list_C\n",
    "        self.list_gamma = list_gamma\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "    \n",
    "    def RF_hyper_parameter_setup(self,x):\n",
    "    \n",
    "   \n",
    "    # Number of trees in random forest\n",
    "        n_estimators = [int(x) for x in np.linspace(self.start_tree, self.stop_tree , self.num_tree)]\n",
    "        # Number of features to consider at every split\n",
    "        max_features = ['auto', 'log2', None]\n",
    "        # Maximum number of levels in tree\n",
    "        Max_depth = [int(x) for x in np.linspace(self.min_depth , self.max_depth , self.num_depth )]\n",
    "        Max_depth.append(None)\n",
    "\n",
    "        random_grid = {'n_estimators': n_estimators,\n",
    "                       'max_features': max_features,\n",
    "                        'min_samples_split': min_samples_split,\n",
    "                      'min_samples_leaf': min_samples_leaf,\n",
    "                       'max_depth' : Max_depth,\n",
    "                      'bootstrap':self.bootstrap}\n",
    "\n",
    "\n",
    "        return random_grid\n",
    "\n",
    "\n",
    "    def RF_hyper_parameter_tuning(self,x,y):\n",
    "    \n",
    "        if norm_condition == 'True':\n",
    "            scaler1 = StandardScaler()\n",
    "            x = scaler1.fit_transform(x)\n",
    "\n",
    "        rf = RandomForestClassifier()\n",
    "        rf_random = RandomizedSearchCV(estimator = rf, param_distributions = self.RF_hyper_parameter_setup(x), n_iter = self.n_iter, cv = self.cv, verbose=2, random_state=42, n_jobs = -1)\n",
    "        rf_random.fit(x, y)\n",
    "        return (rf_random.best_params_)\n",
    "    \n",
    "    def SVM_hyperparameter_tuning(self,x,y):\n",
    "        \n",
    "        if norm_condition == 'True':\n",
    "            scaler1 = StandardScaler()\n",
    "            x = scaler1.fit_transform(x)\n",
    "        \n",
    "        param_grid_svm = {'C': list_C, \n",
    "              'gamma': list_gamma,\n",
    "              'kernel': ['rbf']}\n",
    "        \n",
    "        \n",
    "        clf_random = GridSearchCV(SVC(),param_grid_svm,refit=True,verbose=2)\n",
    "\n",
    "        clf_random.fit(x, y)\n",
    "        return (clf_random.best_params_)\n",
    "    \n",
    "\n",
    "    \n",
    "    def LOOV(self,x,y):\n",
    "        \n",
    "        if norm_condition == 'True':\n",
    "            scaler1 = StandardScaler()\n",
    "            x = scaler1.fit_transform(x)\n",
    "        \n",
    "        lpo = LeavePOut(p=1)\n",
    "        train_ = []\n",
    "        test_ = []\n",
    "        Y_train = []\n",
    "        Y_test = []\n",
    "\n",
    "        for train, test in lpo.split(x):\n",
    "            train_.append(x[train])\n",
    "            test_.append(x[test])\n",
    "\n",
    "        for train, test in lpo.split(y):\n",
    "            Y_train.append(y[train])\n",
    "            Y_test.append(y[test])\n",
    "            \n",
    "        return train_, test_, Y_train, Y_test\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier_training\n",
    "\n",
    "- best_param: set of optimal hyper parameters: this can be achived using the Classifier_hypertuning class. \n",
    "- model_name: should be chosen from 'rf', 'svc' or 'gdb'.\n",
    "- MC_iteration: For noise propagation of input values the number of iterations of Monte Carlo apporach should be selected (integer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier_training:\n",
    "    \n",
    "    def __init__(self, best_param, model_name, MC_iteration):\n",
    "        self.best_param = best_param\n",
    "        self.model_name = model_name\n",
    "        self.MC_iteration = MC_iteration\n",
    "    \n",
    "    def train_model(self,train_, test_, Y_train, Y_test):\n",
    "        \n",
    "        params = self.best_param\n",
    "        if self.model_name == 'rf':\n",
    "            \n",
    "            model = RandomForestClassifier(**params)\n",
    "        elif self.model_name == 'svc':\n",
    "            \n",
    "            model = OutputCodeClassifier(svm.SVC(**params),code_size=20)\n",
    "        elif self.model_name == 'gdb':\n",
    "            \n",
    "            model = OneVsRestClassifier(GradientBoostingClassifier(max_depth = 5, min_samples_split = 5))\n",
    "\n",
    "\n",
    "            \n",
    "        pred_model = []\n",
    "\n",
    "        for i in range(len(train_)):\n",
    "                \n",
    "                model.fit(train_[i], Y_train[i])\n",
    "    \n",
    "                pred_model.append(model.predict(test_[i]))\n",
    "        \n",
    "       \n",
    "    \n",
    "        return pred_model\n",
    "    \n",
    "        \n",
    "    def train_model_added_noise(self,train_, test_, Y_train, Y_test):\n",
    "        \n",
    "        params = self.best_param\n",
    "        if self.model_name == 'rf':\n",
    "            \n",
    "            model = RandomForestClassifier(**params)\n",
    "        elif self.model_name == 'svc':\n",
    "            \n",
    "            model = OutputCodeClassifier(svm.SVC(**params),code_size=20)\n",
    "            \n",
    "        elif self.model_name == 'gdb':\n",
    "            \n",
    "            model = OneVsRestClassifier(GradientBoostingClassifier(max_depth = 5, min_samples_split = 5))\n",
    "\n",
    "\n",
    "       \n",
    "        pred_model_noise = []\n",
    "        for i in range(len(train_)):\n",
    "            for j in range(self.MC_iteration):\n",
    "    \n",
    "                model.fit((train_[i]+ np.random.normal(.01,0)), Y_train[i])\n",
    "    \n",
    "                pred_model_noise.append(model.predict(test_[i]))\n",
    "        \n",
    "        \n",
    "    \n",
    "        return pred_model_noise\n",
    "        \n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of Training\n",
    "\n",
    "The following cell is an example of trainig for support vector calssifier. At first, \"Classifier_hypertuning\" is called. Then the \"best\" parameters are caclulated (model.SVM_hyperparameter_tuning(x,y1)). By running model_tune.LOOV(x,y1), the training and test data sets are generated. The classifer is trained by calling the  Classifier_training class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_tune =  Classifier_hypertuning(min_samples_split,min_samples_leaf,bootstrap,start_tree,stop_tree,num_tree,min_depth,max_depth,num_depth,n_iter,cv,norm_condition, list_C, list_gamma)\n",
    "best_parameter_svm = model.SVM_hyperparameter_tuning(x,y1)\n",
    "train_, test_ ,Y_train, Y_test = model_tune.LOOV(x,y1)\n",
    "model_tune =  Classifier_training(best_parameter_svm, 'svc', 500)\n",
    "result = model_tune.train_model(train_, test_ ,Y_train, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print(\"The accuracy of classifier is:\", np.round(accuracy_score(y1, result),2))\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "target_names = ['N', 'FC', 'FW', 'RC','RW','LC','LW']\n",
    "print(classification_report(y1, result, target_names=target_names))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
